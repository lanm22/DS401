I think Bio187 was a great foundation for my studies in data science. In that class, I learned how to use the best data science python libraries like numpy and pandas. It was also interesting seeing how data science skills can be applied to analyze real-world problems. CSCI111 taught me the most basic python skills and trained me as a programmer. The concepts I mastered in that class reappeared in CSCI112, but in a more advanced way. CSCI112 has not taught me anything new about data science per se, but I learned how data structures are stored in the system, such as arrays and linked structures. I got a chance to appreciate the ease of using these coding interfaces and all the work that goes behind them to make every operation work so smoothly. The statistics course I took for the minor was an engineering statistics class I took at Iowa State University when I was a senior in high school. A lot of the concepts from that class showed up again in bio187, such as poisson distribution and binomial distribution. Although I did not apply these topics in my data science projects, I was able to understand different ways numbers can be distributed and apply this knowledge when I read papers in bio385. I believe that all of the classes for data science overlapped with each other one way or another, and I got to pick the most interesting analytic skills for my own projects.

For the DS project I worked on this semester, I extended the analysis I did in Bio187 by trying out ways to reduce dimensionality and cluster the results. t-SNE is a technique that I first learned about in my summer Stanford project and revisited in a paper we read during Bio385. t-SNE reduces dimensionality while maintaining the relationships between data points. Perplexity is a measure of how to balanece attention between the local and global aspects of the data. I tried a few different numbers (5, 20, and 50) to find a good perplexity value, but none of them are very good. The results I got were all a big blob of dots, which means that there is not enough separation. Then I used k-means to cluster the data. K-means is also a concept I first heard at Stanford then in Bio385. I clustered each of the t-SNE results in to 5 distinct clusters. Overall, the best results were accomplished with 5 clusters and a perplexity of 5.
